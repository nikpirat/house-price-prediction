import joblib
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
from src.config import MODEL_PATH

def evaluate_model(X_test, y_test, print_samples=True):
    model = joblib.load(MODEL_PATH)
    y_pred = model.predict(X_test)

    mse = mean_squared_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)
    mae = mean_absolute_error(y_test, y_pred)

    print("\nðŸ“Š Model Evaluation on Test Set:")
    print(f"ðŸ”¸ MSE  (Mean Squared Error): {mse:.2f}") # Shows average of the squared differences between predicted and actual prices.
    # Error size, penalizes big ones
    # Unit: Squared target units
    # Smaller is better

    print(f"ðŸ”¸ MAE  (Mean Absolute Error): {mae:.2f}") # Shows average of the absolute difference between predicted and actual prices.
    # Average error size
    # Unit: Same as target (e.g., $)
    # Smaller is better

    print(f"ðŸ”¸ RÂ² Score: {r2:.4f}") # Coefficient of Determination - Measures how well your model explains the variance in the target variable.
    # RÂ² = 0.8206 â†’ your model explains 82% of the variability in house prices.
    # RÂ² = 1 â†’ perfect model
    # RÂ² = 0 â†’ no better than predicting the mean
    # RÂ² < 0 â†’ worse than mean prediction
    # Overall effectiveness score

    if print_samples:
        print("\nðŸ§ª Sample Predictions vs Actual:")
        for i in range(5):
            print(f"  ðŸ”¹ Predicted: {y_pred[i]:,.2f}  |  Actual: {y_test.iloc[i]:,.2f}")
